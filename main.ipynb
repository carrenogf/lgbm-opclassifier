{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador OP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from flow import preprocessing, training, testing, preprocessing_predict, predict\n",
    "## Parametros base\n",
    "STUDY_NAME = \"lgbm_kappa_bbdd_2\"\n",
    "STUDY_NAME = STUDY_NAME.replace(\" \",\"_\")\n",
    "BASEDIR = os.getcwd() \n",
    "\n",
    "# Carpetas base\n",
    "os.makedirs(os.path.join(BASEDIR,\"work\"),exist_ok=True) # crear la carpeta work\n",
    "os.makedirs(os.path.join(BASEDIR,\"flow\"),exist_ok=True) # crear la carpeta flow\n",
    "os.makedirs(os.path.join(BASEDIR,\"work\",STUDY_NAME),exist_ok=True) # crear la carpeta del estudio nuevo\n",
    "\n",
    "PATH_WORK = os.path.join(os.getcwd(),\"work\",STUDY_NAME)\n",
    "PATH_FLOW = os.path.join(os.getcwd(),\"flow\")\n",
    "\n",
    "# work\n",
    "PATH_PREPROCESSING = os.path.join(PATH_WORK,\"preprocessing\")\n",
    "PATH_TRAINING = os.path.join(PATH_WORK,\"training\")\n",
    "PATH_MODELS = os.path.join(PATH_WORK,\"models\")\n",
    "PATH_PREDICT = os.path.join(PATH_WORK,\"predict\")\n",
    "PATH_RESULTS = os.path.join(PATH_WORK,\"results\")\n",
    "\n",
    "os.makedirs(PATH_PREPROCESSING,exist_ok=True)\n",
    "os.makedirs(PATH_TRAINING,exist_ok=True)\n",
    "os.makedirs(PATH_MODELS,exist_ok=True)\n",
    "os.makedirs(PATH_PREDICT,exist_ok=True)\n",
    "os.makedirs(PATH_RESULTS,exist_ok=True)\n",
    "\n",
    "bbdd_name = \"optuna.sqlite3\"\n",
    "path_optuna = os.path.join(os.getcwd(),\"work\",bbdd_name)\n",
    "BBDD_OPTUNA = f\"sqlite:///{Path(path_optuna).as_posix()}\"\n",
    "\n",
    "DATASET_BASE = \"dt_train.xlsx\"\n",
    "FEATURES_BASE = ['corrent',\n",
    " 'nom_entidad1',\n",
    " 'cod_cta_banco',\n",
    " 'nom_cta',\n",
    " 'clase_reg',\n",
    " 'cod_banco',\n",
    " 'clase_cta',\n",
    " 'descripcion',\n",
    " 'clase_gasto',\n",
    " 'codigo_acr1',\n",
    " 'codfte1',\n",
    " 'tipo_comprobante2',\n",
    " 'cuit',\n",
    " 'glosa1',\n",
    " 'clase']\n",
    "\n",
    "# VARIABLES CATEGORICAS PARA ONEHOT ENCODING\n",
    "FEATURES_CATEG = ['tipo_comprobante2','clase_reg','clase_gasto','clase_cta']\n",
    "\n",
    "params_base = {}\n",
    "# study\n",
    "params_base[\"STUDY_NAME\"] = STUDY_NAME\n",
    "params_base[\"BBDD_OPTUNA\"] = BBDD_OPTUNA\n",
    "params_base[\"TABLE\"] = \"mesa_entrada_clase\"\n",
    "params_base[\"TABLE_DEUDA\"] = \"mesa_entrada_deuda\"\n",
    "# carpetas\n",
    "params_base[\"BASEDIR\"] = BASEDIR\n",
    "params_base[\"PATH_WORK\"] = PATH_WORK\n",
    "params_base[\"PATH_FLOW\"] = PATH_FLOW\n",
    "params_base[\"PATH_PREPROCESSING\"] = PATH_PREPROCESSING\n",
    "params_base[\"PATH_TRAINING\"] = PATH_TRAINING\n",
    "params_base[\"PATH_MODELS\"] = PATH_MODELS\n",
    "\n",
    "# datasets\n",
    "params_base[\"DATASET_BASE\"] = os.path.join(BASEDIR,DATASET_BASE)\n",
    "params_base[\"DATASET_PROCESSED\"] = os.path.join(PATH_TRAINING,'dt_train_processed.csv')\n",
    "params_base[\"FEATURES_BASE\"] = FEATURES_BASE\n",
    "params_base[\"FEATURES_CATEG\"] = FEATURES_CATEG\n",
    "params_base[\"DATASET_PREDICT\"] = os.path.join(PATH_PREDICT,'dt_pred_orig.csv')\n",
    "params_base[\"DATASET_PREDICT_PROCESSED\"] = os.path.join(PATH_PREDICT,'dt_prep_processed.csv')\n",
    "\n",
    "# preprocess\n",
    "path_preprocess_params = os.path.join(params_base[\"PATH_WORK\"],f'params_preprocessing.json')\n",
    "params_base[\"PREPROCESSING_PARAMS\"] = path_preprocess_params\n",
    "# Training \n",
    "params_base[\"MODEL\"] = f'{PATH_MODELS}/model_{STUDY_NAME}.pkl'\n",
    "params_base[\"SEED\"] = 12345\n",
    "params_base[\"TEST_SIZE\"] = 0.2\n",
    "params_base[\"N_TRIALS\"] = 100\n",
    "params_base[\"PARAMS_INT\"] = ['num_leaves','max_depth','min_child_samples']\n",
    "params_base[\"PARAMS_UNIFORM\"] = ['num_leaves','max_depth']\n",
    "params_base[\"PARAMS_LOGUNIFORM\"] = ['learning_rate','reg_alpha','reg_lambda']\n",
    "params_base[\"HP\"] = {\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',  # Esto es solo para LightGBM; la métrica de optimización será kappa\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': (1e-3, 1e-1),\n",
    "    'num_leaves': (31, 256),\n",
    "    'max_depth': (-1, 15),\n",
    "    'min_child_samples': (5, 100),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0),\n",
    "    'reg_alpha': ( 1e-8, 1.0),\n",
    "    'reg_lambda': (1e-8, 1.0),\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "\n",
    "path_params_base = f'work/{STUDY_NAME}/paramas_base.json'\n",
    "with open(path_params_base, 'w') as file:\n",
    "    json.dump(params_base, file, indent=4)\n",
    "\n",
    "# crear un bat para ejecutar el optuna dashboard\n",
    "with open(os.path.join(os.getcwd(),\"work\",\"optuna.bat\"),'w') as optunabat:\n",
    "    optunabat.write(f\"optuna-dashboard sqlite:///{bbdd_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fcarreno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargan datos de las BBDD\n",
      "Se cargan datos locales\n",
      "Onehot encoding finalizado\n",
      "Procesando texto\n",
      "Procesamiento de texto finalizado\n",
      "Parametros de preprocesamiento guardados\n",
      "Dataframe preprocesado guardado\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Preprocesamiento finalizado'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprocesamiento\n",
    "shutil.copy(DATASET_BASE, PATH_PREPROCESSING)\n",
    "preprocessing.preprocess(path_params_base=path_params_base, path_train_local = f'{PATH_PREPROCESSING}\\\\{DATASET_BASE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entrenamiento\n",
    "## para 100 trials demora 1 hora\n",
    "training.train(path_params_base=path_params_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "testing.test(path_params_base=path_params_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fcarreno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargan datos de las BBDD\n",
      "Onehot encoding finalizado\n",
      "Procesando texto\n",
      "Procesamiento de texto finalizado\n",
      "Dataframe procesado guardado\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Preprocesamiento finalizado'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "preprocessing_predict.preprocess_predict(path_params_base=path_params_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo lgbm_kappa_bbdd_2 cargado\n",
      "Proceso Terminado\n"
     ]
    }
   ],
   "source": [
    "# predict final\n",
    "import importlib\n",
    "importlib.reload(predict)\n",
    "now = datetime.now().strftime(\"%d-%m-%Y %H%M\")\n",
    "predict.predict(path_params_base=path_params_base, dataset_final = os.path.join(PATH_RESULTS,f'dt_final {now}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
